{
  "id": "cks-i-008",
  "title": "Audit Logging and Security Event Monitoring",
  "description": "Configure comprehensive audit logging and security event monitoring:\n\n1. Configure Kubernetes audit logging with the following policy:\n   - Log all ||create||, ||update||, ||delete|| operations at ||RequestResponse|| level\n   - Log ||get|| and ||list|| operations for secrets at ||Metadata|| level\n   - Log pod ||exec|| and ||attach|| operations at ||Request|| level\n   - Exclude system components and health checks\n\n2. Set up log aggregation in ||mars|| namespace with:\n   - Fluentd daemonset for log collection\n   - ConfigMap with audit log parsing rules\n   - Service to expose log aggregation endpoint\n\n3. Create security monitoring alerts for:\n   - Failed authentication attempts\n   - Privilege escalation attempts\n   - Unauthorized secret access\n   - Pod exec operations in production namespaces\n\n4. Deploy a test scenario that triggers audit events:\n   - Create a pod named ||audit-test|| that attempts privilege escalation\n   - Try to access secrets from unauthorized service account\n   - Execute commands in the pod\n\n5. Verify audit logs are captured and formatted correctly",
  "points": 9,
  "timeLimit": 18,
  "category": "Audit and Monitoring",
  "tags": [
    "audit-logging",
    "security-monitoring",
    "fluentd",
    "log-aggregation",
    "security-events",
    "intermediate"
  ],
  "infrastructure": {
    "namespaces": [
      "mars"
    ],
    "resources": [
      "configmaps",
      "daemonsets",
      "services",
      "pods"
    ],
    "prerequisites": [
      "Audit logging enabled on API server",
      "Log aggregation infrastructure"
    ]
  },
  "solution": {
    "steps": [
      "1. Create audit policy configuration:",
      "   cat > audit-policy.yaml << EOF",
      "   apiVersion: audit.k8s.io/v1",
      "   kind: Policy",
      "   rules:",
      "   # Log pod exec and attach operations",
      "   - level: Request",
      "     resources:",
      "     - group: ''",
      "       resources: ['pods/exec', 'pods/attach']",
      "   # Log secret access",
      "   - level: Metadata",
      "     resources:",
      "     - group: ''",
      "       resources: ['secrets']",
      "     verbs: ['get', 'list']",
      "   # Log all modifications at RequestResponse level",
      "   - level: RequestResponse",
      "     verbs: ['create', 'update', 'patch', 'delete']",
      "     resources:",
      "     - group: ''",
      "     - group: 'apps'",
      "     - group: 'networking.k8s.io'",
      "   # Exclude system components",
      "   - level: None",
      "     users: ['system:kube-proxy', 'system:kube-scheduler', 'system:kube-controller-manager']",
      "   # Exclude health checks",
      "   - level: None",
      "     resources:",
      "     - group: ''",
      "       resources: ['events']",
      "   EOF",
      "2. Create audit policy ConfigMap:",
      "   kubectl create configmap audit-policy --from-file=audit-policy.yaml -n kube-system",
      "3. Create Fluentd configuration:",
      "   cat > fluentd-audit-config.yaml << EOF",
      "   apiVersion: v1",
      "   kind: ConfigMap",
      "   metadata:",
      "     name: fluentd-audit-config",
      "     namespace: mars",
      "   data:",
      "     fluent.conf: |",
      "       <source>",
      "         @type tail",
      "         path /var/log/audit.log",
      "         pos_file /var/log/fluentd-audit.log.pos",
      "         tag kubernetes.audit",
      "         format json",
      "         time_key timestamp",
      "         time_format %Y-%m-%dT%H:%M:%S.%NZ",
      "         keep_time_key true",
      "       </source>",
      "       ",
      "       <filter kubernetes.audit>",
      "         @type grep",
      "         <regexp>",
      "           key verb",
      "           pattern ^(create|update|delete|exec|attach)$",
      "         </regexp>",
      "       </filter>",
      "       ",
      "       <match kubernetes.audit>",
      "         @type forward",
      "         <server>",
      "           name security-logs",
      "           host audit-collector.mars.svc.cluster.local",
      "           port 24224",
      "         </server>",
      "       </match>",
      "   EOF",
      "4. Apply Fluentd config:",
      "   kubectl apply -f fluentd-audit-config.yaml",
      "5. Create Fluentd DaemonSet:",
      "   cat > fluentd-daemonset.yaml << EOF",
      "   apiVersion: apps/v1",
      "   kind: DaemonSet",
      "   metadata:",
      "     name: fluentd-audit",
      "     namespace: mars",
      "   spec:",
      "     selector:",
      "       matchLabels:",
      "         app: fluentd-audit",
      "     template:",
      "       metadata:",
      "         labels:",
      "           app: fluentd-audit",
      "       spec:",
      "         serviceAccountName: fluentd",
      "         tolerations:",
      "         - key: node-role.kubernetes.io/control-plane",
      "           effect: NoSchedule",
      "         - key: node-role.kubernetes.io/master",
      "           effect: NoSchedule",
      "         containers:",
      "         - name: fluentd",
      "           image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch",
      "           env:",
      "           - name: FLUENTD_CONF",
      "             value: 'fluent.conf'",
      "           volumeMounts:",
      "           - name: config",
      "             mountPath: /fluentd/etc",
      "           - name: varlog",
      "             mountPath: /var/log",
      "           securityContext:",
      "             runAsUser: 0",
      "           resources:",
      "             limits:",
      "               memory: 200Mi",
      "               cpu: 100m",
      "         volumes:",
      "         - name: config",
      "           configMap:",
      "             name: fluentd-audit-config",
      "         - name: varlog",
      "           hostPath:",
      "             path: /var/log",
      "   EOF",
      "6. Create Fluentd ServiceAccount:",
      "   kubectl create serviceaccount fluentd -n mars",
      "   kubectl create clusterrole fluentd --verb=get,list,watch --resource=pods,namespaces",
      "   kubectl create clusterrolebinding fluentd --clusterrole=fluentd --serviceaccount=mars:fluentd",
      "7. Apply Fluentd DaemonSet:",
      "   kubectl apply -f fluentd-daemonset.yaml",
      "8. Create audit test pod with privilege escalation attempt:",
      "   cat > audit-test-pod.yaml << EOF",
      "   apiVersion: v1",
      "   kind: Pod",
      "   metadata:",
      "     name: audit-test",
      "     namespace: mars",
      "   spec:",
      "     securityContext:",
      "       runAsUser: 0",
      "     containers:",
      "     - name: test",
      "       image: alpine:latest",
      "       command: ['sleep', '3600']",
      "       securityContext:",
      "         privileged: true",
      "         allowPrivilegeEscalation: true",
      "   EOF",
      "9. Apply test pod (should trigger audit events):",
      "   kubectl apply -f audit-test-pod.yaml",
      "10. Test exec operation (triggers audit):",
      "    kubectl exec audit-test -n mars -- whoami",
      "11. Verify Fluentd is collecting logs:",
      "    kubectl get pods -l app=fluentd-audit -n mars",
      "    kubectl logs -l app=fluentd-audit -n mars | head -10"
    ]
  },
  "validations": [
    {
      "command": "kubectl get configmap audit-policy -n kube-system -o jsonpath='{.metadata.name}'",
      "expected": "audit-policy",
      "points": 1,
      "description": "Audit policy ConfigMap should exist"
    },
    {
      "command": "kubectl get configmap fluentd-audit-config -n mars -o jsonpath='{.metadata.name}'",
      "expected": "fluentd-audit-config",
      "points": 1,
      "description": "Fluentd audit config should exist"
    },
    {
      "command": "kubectl get serviceaccount fluentd -n mars -o jsonpath='{.metadata.name}'",
      "expected": "fluentd",
      "points": 1,
      "description": "Fluentd ServiceAccount should exist"
    },
    {
      "command": "kubectl get daemonset fluentd-audit -n mars -o jsonpath='{.metadata.name}'",
      "expected": "fluentd-audit",
      "points": 2,
      "description": "Fluentd DaemonSet should exist"
    },
    {
      "command": "kubectl get pod audit-test -n mars -o jsonpath='{.metadata.name}'",
      "expected": "audit-test",
      "points": 1,
      "description": "Audit test pod should exist"
    },
    {
      "command": "kubectl get pod audit-test -n mars -o jsonpath='{.spec.containers[0].securityContext.privileged}'",
      "expected": "true",
      "points": 1,
      "description": "Test pod should have privileged security context"
    },
    {
      "command": "kubectl get daemonset fluentd-audit -n mars -o jsonpath='{.status.numberReady}'",
      "expected": "[1-9][0-9]*",
      "points": 1,
      "description": "Fluentd DaemonSet should have ready pods"
    },
    {
      "command": "kubectl get clusterrolebinding fluentd -o jsonpath='{.metadata.name}'",
      "expected": "fluentd",
      "points": 1,
      "description": "Fluentd ClusterRoleBinding should exist"
    }
  ]
}