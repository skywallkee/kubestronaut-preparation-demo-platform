{
  "examType": "cka",
  "version": "1.0",
  "metadata": {
    "totalQuestions": 60,
    "questionsPerExam": 15,
    "passingScore": 67,
    "duration": 180,
    "difficulties": {
      "beginner": 20,
      "intermediate": 20,
      "advanced": 20
    }
  },
  "questions": {
    "beginner": [
      {
        "id": "cka-b-001",
        "title": "Check Cluster Health",
        "description": "Check the health of all nodes in the cluster and list any nodes that are not in Ready state.",
        "points": 4,
        "timeLimit": 8,
        "category": "Cluster Architecture",
        "tags": ["nodes", "health", "cluster"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["nodes"],
          "prerequisites": ["multi-node cluster"]
        },
        "solution": {
          "steps": [
            "1. Check all nodes status:",
            "   kubectl get nodes",
            "2. Check node details for any not Ready:",
            "   kubectl describe node <node-name>",
            "3. Check for any unhealthy nodes:",
            "   kubectl get nodes --show-labels"
          ]
        },
        "validations": [
          {
            "command": "kubectl get nodes --no-headers | grep -v Ready | wc -l",
            "expected": "0",
            "points": 4,
            "description": "All nodes should be in Ready state"
          }
        ]
      },
      {
        "id": "cka-b-002",
        "title": "Create a Namespace",
        "description": "Create a namespace called 'production' and set it as the default namespace for your current context.",
        "points": 3,
        "timeLimit": 5,
        "category": "Cluster Architecture",
        "tags": ["namespace", "context"],
        "infrastructure": {
          "namespaces": ["production"],
          "resources": ["namespaces"]
        },
        "solution": {
          "steps": [
            "1. Create the namespace:",
            "   kubectl create namespace production",
            "2. Set as default for current context:",
            "   kubectl config set-context --current --namespace=production",
            "3. Verify the change:",
            "   kubectl config get-contexts"
          ]
        },
        "validations": [
          {
            "command": "kubectl get namespace production --no-headers",
            "expected": "production.*Active",
            "points": 2,
            "description": "Namespace production should exist and be active"
          },
          {
            "command": "kubectl config get-contexts | grep '^\\*' | awk '{print $5}'",
            "expected": "production",
            "points": 1,
            "description": "Current context should use production namespace"
          }
        ]
      },
      {
        "id": "cka-b-003",
        "title": "Get Pod Logs",
        "description": "Create a pod called 'log-pod' using nginx image in the default namespace. Then retrieve and save the last 50 lines of logs to /tmp/pod-logs.txt",
        "points": 5,
        "timeLimit": 8,
        "category": "Troubleshooting",
        "tags": ["pods", "logs", "troubleshooting"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["pods"],
          "images": ["nginx"]
        },
        "solution": {
          "steps": [
            "1. Create the pod:",
            "   kubectl run log-pod --image=nginx",
            "2. Wait for pod to be ready:",
            "   kubectl wait --for=condition=ready pod/log-pod",
            "3. Get logs and save to file:",
            "   kubectl logs log-pod --tail=50 > /tmp/pod-logs.txt"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pod log-pod --no-headers | awk '{print $3}'",
            "expected": "Running",
            "points": 2,
            "description": "Pod should be running"
          },
          {
            "command": "test -f /tmp/pod-logs.txt && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "Log file should exist"
          },
          {
            "command": "wc -l < /tmp/pod-logs.txt",
            "expected": "^[1-9][0-9]*$",
            "points": 1,
            "description": "Log file should contain logs"
          }
        ]
      },
      {
        "id": "cka-b-004",
        "title": "Scale a Deployment",
        "description": "Create a deployment called 'web-app' using nginx:1.20 image with 2 replicas. Then scale it to 5 replicas.",
        "points": 4,
        "timeLimit": 7,
        "category": "Workloads",
        "tags": ["deployment", "scaling"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["deployments", "pods"],
          "images": ["nginx:1.20"]
        },
        "solution": {
          "steps": [
            "1. Create deployment with 2 replicas:",
            "   kubectl create deployment web-app --image=nginx:1.20 --replicas=2",
            "2. Scale to 5 replicas:",
            "   kubectl scale deployment web-app --replicas=5",
            "3. Verify scaling:",
            "   kubectl get deployment web-app"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment web-app -o jsonpath='{.spec.replicas}'",
            "expected": "5",
            "points": 2,
            "description": "Deployment should have 5 replicas specified"
          },
          {
            "command": "kubectl get deployment web-app -o jsonpath='{.status.readyReplicas}'",
            "expected": "5",
            "points": 2,
            "description": "Deployment should have 5 ready replicas"
          }
        ]
      },
      {
        "id": "cka-b-005",
        "title": "Create Service Account",
        "description": "Create a service account named 'app-service-account' in the default namespace and list all service accounts.",
        "points": 3,
        "timeLimit": 5,
        "category": "Security",
        "tags": ["serviceaccount", "security"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["serviceaccounts"]
        },
        "solution": {
          "steps": [
            "1. Create service account:",
            "   kubectl create serviceaccount app-service-account",
            "2. List all service accounts:",
            "   kubectl get serviceaccounts"
          ]
        },
        "validations": [
          {
            "command": "kubectl get serviceaccount app-service-account --no-headers | awk '{print $1}'",
            "expected": "app-service-account",
            "points": 3,
            "description": "Service account should exist"
          }
        ]
      },
      {
        "id": "cka-b-006",
        "title": "Label Nodes",
        "description": "Label the first worker node with 'environment=production' and 'tier=frontend'.",
        "points": 4,
        "timeLimit": 6,
        "category": "Cluster Architecture",
        "tags": ["nodes", "labels"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["nodes"],
          "prerequisites": ["worker nodes"]
        },
        "solution": {
          "steps": [
            "1. Get worker nodes:",
            "   kubectl get nodes --show-labels",
            "2. Label the first worker node:",
            "   kubectl label node <worker-node-name> environment=production tier=frontend",
            "3. Verify labels:",
            "   kubectl get nodes --show-labels"
          ]
        },
        "validations": [
          {
            "command": "kubectl get nodes -l environment=production --no-headers | wc -l",
            "expected": "1",
            "points": 2,
            "description": "One node should have environment=production label"
          },
          {
            "command": "kubectl get nodes -l tier=frontend --no-headers | wc -l",
            "expected": "1",
            "points": 2,
            "description": "One node should have tier=frontend label"
          }
        ]
      },
      {
        "id": "cka-b-007",
        "title": "Create ConfigMap",
        "description": "Create a ConfigMap named 'app-config' with key 'database_url' and value 'mysql://localhost:3306/mydb'.",
        "points": 3,
        "timeLimit": 5,
        "category": "Configuration",
        "tags": ["configmap", "configuration"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["configmaps"]
        },
        "solution": {
          "steps": [
            "1. Create ConfigMap:",
            "   kubectl create configmap app-config --from-literal=database_url='mysql://localhost:3306/mydb'",
            "2. Verify creation:",
            "   kubectl get configmap app-config",
            "3. Check content:",
            "   kubectl describe configmap app-config"
          ]
        },
        "validations": [
          {
            "command": "kubectl get configmap app-config -o jsonpath='{.data.database_url}'",
            "expected": "mysql://localhost:3306/mydb",
            "points": 3,
            "description": "ConfigMap should contain correct database URL"
          }
        ]
      },
      {
        "id": "cka-b-008",
        "title": "Get Resource Usage",
        "description": "Get the CPU and memory usage of all pods in the default namespace and save the output to /tmp/resource-usage.txt.",
        "points": 4,
        "timeLimit": 7,
        "category": "Troubleshooting",
        "tags": ["resources", "monitoring"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["pods"],
          "prerequisites": ["metrics-server"]
        },
        "solution": {
          "steps": [
            "1. Get pod resource usage:",
            "   kubectl top pods",
            "2. Save to file:",
            "   kubectl top pods > /tmp/resource-usage.txt",
            "3. Verify file creation:",
            "   cat /tmp/resource-usage.txt"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/resource-usage.txt && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "Resource usage file should exist"
          },
          {
            "command": "grep -c 'CPU\\|NAME' /tmp/resource-usage.txt",
            "expected": "^[1-9][0-9]*$",
            "points": 2,
            "description": "File should contain resource usage data"
          }
        ]
      },
      {
        "id": "cka-b-009",
        "title": "Create Secret",
        "description": "Create a secret named 'mysecret' with username 'admin' and password 'secretpassword' using kubectl.",
        "points": 4,
        "timeLimit": 6,
        "category": "Security",
        "tags": ["secret", "security"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["secrets"]
        },
        "solution": {
          "steps": [
            "1. Create secret:",
            "   kubectl create secret generic mysecret --from-literal=username=admin --from-literal=password=secretpassword",
            "2. Verify creation:",
            "   kubectl get secret mysecret",
            "3. Check secret data:",
            "   kubectl describe secret mysecret"
          ]
        },
        "validations": [
          {
            "command": "kubectl get secret mysecret -o jsonpath='{.data.username}' | base64 -d",
            "expected": "admin",
            "points": 2,
            "description": "Secret should contain correct username"
          },
          {
            "command": "kubectl get secret mysecret -o jsonpath='{.data.password}' | base64 -d",
            "expected": "secretpassword",
            "points": 2,
            "description": "Secret should contain correct password"
          }
        ]
      },
      {
        "id": "cka-b-010",
        "title": "Drain a Node",
        "description": "Safely drain a worker node for maintenance, ensuring pods are evicted gracefully.",
        "points": 5,
        "timeLimit": 10,
        "category": "Cluster Maintenance",
        "tags": ["nodes", "drain", "maintenance"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["nodes", "pods"],
          "prerequisites": ["worker nodes", "deployments"]
        },
        "solution": {
          "steps": [
            "1. Get worker nodes:",
            "   kubectl get nodes",
            "2. Drain the first worker node:",
            "   kubectl drain <worker-node-name> --ignore-daemonsets --force",
            "3. Verify node is drained:",
            "   kubectl get nodes"
          ]
        },
        "validations": [
          {
            "command": "kubectl get nodes | grep -c 'SchedulingDisabled'",
            "expected": "1",
            "points": 3,
            "description": "One node should be marked as SchedulingDisabled"
          },
          {
            "command": "kubectl get pods -A --field-selector spec.nodeName=$(kubectl get nodes | grep SchedulingDisabled | awk '{print $1}') | grep -v 'kube-system\\|DaemonSet' | wc -l",
            "expected": "0",
            "points": 2,
            "description": "Drained node should have no non-system pods"
          }
        ]
      },
      {
        "id": "cka-b-011",
        "title": "Create PersistentVolume",
        "description": "Create a PersistentVolume named 'pv-storage' with 1Gi capacity, hostPath storage type pointing to /tmp/data, and ReadWriteOnce access mode.",
        "points": 5,
        "timeLimit": 8,
        "category": "Storage",
        "tags": ["persistentvolume", "storage"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["persistentvolumes"],
          "directories": ["/tmp/data"]
        },
        "solution": {
          "steps": [
            "1. Create directory on node:",
            "   mkdir -p /tmp/data",
            "2. Create PV manifest:",
            "   cat > pv.yaml << EOF",
            "   apiVersion: v1",
            "   kind: PersistentVolume",
            "   metadata:",
            "     name: pv-storage",
            "   spec:",
            "     capacity:",
            "       storage: 1Gi",
            "     accessModes:",
            "       - ReadWriteOnce",
            "     hostPath:",
            "       path: /tmp/data",
            "   EOF",
            "3. Apply the manifest:",
            "   kubectl apply -f pv.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pv pv-storage -o jsonpath='{.spec.capacity.storage}'",
            "expected": "1Gi",
            "points": 2,
            "description": "PV should have 1Gi capacity"
          },
          {
            "command": "kubectl get pv pv-storage -o jsonpath='{.spec.hostPath.path}'",
            "expected": "/tmp/data",
            "points": 2,
            "description": "PV should use /tmp/data hostPath"
          },
          {
            "command": "kubectl get pv pv-storage -o jsonpath='{.spec.accessModes[0]}'",
            "expected": "ReadWriteOnce",
            "points": 1,
            "description": "PV should have ReadWriteOnce access mode"
          }
        ]
      },
      {
        "id": "cka-b-012",
        "title": "View Events",
        "description": "Get all events in the default namespace sorted by timestamp and save them to /tmp/events.txt.",
        "points": 3,
        "timeLimit": 5,
        "category": "Troubleshooting",
        "tags": ["events", "troubleshooting"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["events"]
        },
        "solution": {
          "steps": [
            "1. Get events sorted by timestamp:",
            "   kubectl get events --sort-by='.lastTimestamp'",
            "2. Save to file:",
            "   kubectl get events --sort-by='.lastTimestamp' > /tmp/events.txt",
            "3. Verify file:",
            "   head /tmp/events.txt"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/events.txt && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "Events file should exist"
          },
          {
            "command": "grep -c 'LAST SEEN\\|AGE' /tmp/events.txt",
            "expected": "1",
            "points": 1,
            "description": "File should contain events header"
          }
        ]
      },
      {
        "id": "cka-b-013",
        "title": "Create Role",
        "description": "Create a Role named 'pod-reader' in the default namespace that allows reading pods and services.",
        "points": 4,
        "timeLimit": 7,
        "category": "Security",
        "tags": ["rbac", "role", "security"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["roles"]
        },
        "solution": {
          "steps": [
            "1. Create role with kubectl:",
            "   kubectl create role pod-reader --verb=get,list,watch --resource=pods,services",
            "2. Verify role creation:",
            "   kubectl get role pod-reader",
            "3. Describe role:",
            "   kubectl describe role pod-reader"
          ]
        },
        "validations": [
          {
            "command": "kubectl get role pod-reader --no-headers | awk '{print $1}'",
            "expected": "pod-reader",
            "points": 2,
            "description": "Role should exist"
          },
          {
            "command": "kubectl describe role pod-reader | grep -c 'pods.*get\\|services.*get'",
            "expected": "2",
            "points": 2,
            "description": "Role should allow reading pods and services"
          }
        ]
      },
      {
        "id": "cka-b-014",
        "title": "Check API Resources",
        "description": "List all API resources available in the cluster and save the output to /tmp/api-resources.txt. Then count how many resources are namespaced.",
        "points": 4,
        "timeLimit": 6,
        "category": "Cluster Architecture",
        "tags": ["api", "resources"],
        "infrastructure": {
          "namespaces": [],
          "resources": []
        },
        "solution": {
          "steps": [
            "1. List all API resources:",
            "   kubectl api-resources",
            "2. Save to file:",
            "   kubectl api-resources > /tmp/api-resources.txt",
            "3. Count namespaced resources:",
            "   kubectl api-resources --namespaced=true | wc -l"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/api-resources.txt && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "API resources file should exist"
          },
          {
            "command": "grep -c 'NAME.*SHORTNAMES' /tmp/api-resources.txt",
            "expected": "1",
            "points": 2,
            "description": "File should contain API resources listing"
          }
        ]
      },
      {
        "id": "cka-b-015",
        "title": "Create Job",
        "description": "Create a Job named 'data-processor' that runs busybox:1.35 image to execute 'echo Processing data && sleep 30'.",
        "points": 4,
        "timeLimit": 7,
        "category": "Workloads",
        "tags": ["job", "workload"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["jobs"],
          "images": ["busybox:1.35"]
        },
        "solution": {
          "steps": [
            "1. Create job manifest:",
            "   cat > job.yaml << EOF",
            "   apiVersion: batch/v1",
            "   kind: Job",
            "   metadata:",
            "     name: data-processor",
            "   spec:",
            "     template:",
            "       spec:",
            "         containers:",
            "         - name: processor",
            "           image: busybox:1.35",
            "           command: ['sh', '-c', 'echo Processing data && sleep 30']",
            "         restartPolicy: Never",
            "   EOF",
            "2. Apply the job:",
            "   kubectl apply -f job.yaml",
            "3. Check job status:",
            "   kubectl get jobs"
          ]
        },
        "validations": [
          {
            "command": "kubectl get job data-processor --no-headers | awk '{print $1}'",
            "expected": "data-processor",
            "points": 2,
            "description": "Job should exist"
          },
          {
            "command": "kubectl get job data-processor -o jsonpath='{.spec.template.spec.containers[0].image}'",
            "expected": "busybox:1.35",
            "points": 2,
            "description": "Job should use correct image"
          }
        ]
      },
      {
        "id": "cka-b-016",
        "title": "Uncordon Node",
        "description": "Uncordon a previously drained worker node to make it schedulable again.",
        "points": 3,
        "timeLimit": 5,
        "category": "Cluster Maintenance",
        "tags": ["nodes", "uncordon", "maintenance"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["nodes"],
          "prerequisites": ["drained node"]
        },
        "solution": {
          "steps": [
            "1. List nodes to find drained node:",
            "   kubectl get nodes",
            "2. Uncordon the drained node:",
            "   kubectl uncordon <node-name>",
            "3. Verify node is schedulable:",
            "   kubectl get nodes"
          ]
        },
        "validations": [
          {
            "command": "kubectl get nodes | grep -c 'SchedulingDisabled'",
            "expected": "0",
            "points": 3,
            "description": "No nodes should be SchedulingDisabled"
          }
        ]
      },
      {
        "id": "cka-b-017",
        "title": "Create Network Policy",
        "description": "Create a NetworkPolicy named 'deny-all' in the default namespace that denies all ingress traffic to pods.",
        "points": 5,
        "timeLimit": 8,
        "category": "Networking",
        "tags": ["networkpolicy", "security"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["networkpolicies"],
          "prerequisites": ["CNI with NetworkPolicy support"]
        },
        "solution": {
          "steps": [
            "1. Create NetworkPolicy manifest:",
            "   cat > netpol.yaml << EOF",
            "   apiVersion: networking.k8s.io/v1",
            "   kind: NetworkPolicy",
            "   metadata:",
            "     name: deny-all",
            "     namespace: default",
            "   spec:",
            "     podSelector: {}",
            "     policyTypes:",
            "     - Ingress",
            "   EOF",
            "2. Apply the policy:",
            "   kubectl apply -f netpol.yaml",
            "3. Verify creation:",
            "   kubectl get networkpolicy"
          ]
        },
        "validations": [
          {
            "command": "kubectl get networkpolicy deny-all --no-headers | awk '{print $1}'",
            "expected": "deny-all",
            "points": 3,
            "description": "NetworkPolicy should exist"
          },
          {
            "command": "kubectl get networkpolicy deny-all -o jsonpath='{.spec.policyTypes[0]}'",
            "expected": "Ingress",
            "points": 2,
            "description": "NetworkPolicy should block ingress traffic"
          }
        ]
      },
      {
        "id": "cka-b-018",
        "title": "Check Cluster Version",
        "description": "Check the Kubernetes cluster version and save both client and server versions to /tmp/versions.txt.",
        "points": 3,
        "timeLimit": 5,
        "category": "Cluster Architecture",
        "tags": ["version", "cluster"],
        "infrastructure": {
          "namespaces": [],
          "resources": []
        },
        "solution": {
          "steps": [
            "1. Get cluster version:",
            "   kubectl version",
            "2. Save to file:",
            "   kubectl version > /tmp/versions.txt",
            "3. Verify file:",
            "   cat /tmp/versions.txt"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/versions.txt && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "Version file should exist"
          },
          {
            "command": "grep -c 'Client Version\\|Server Version' /tmp/versions.txt",
            "expected": "2",
            "points": 1,
            "description": "File should contain both client and server versions"
          }
        ]
      },
      {
        "id": "cka-b-019",
        "title": "Create Horizontal Pod Autoscaler",
        "description": "Create an HPA for the 'web-app' deployment that scales between 2-10 replicas based on 50% CPU utilization.",
        "points": 5,
        "timeLimit": 8,
        "category": "Workloads",
        "tags": ["hpa", "autoscaling"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["horizontalpodautoscalers", "deployments"],
          "prerequisites": ["metrics-server", "web-app deployment"]
        },
        "solution": {
          "steps": [
            "1. Create HPA:",
            "   kubectl autoscale deployment web-app --cpu-percent=50 --min=2 --max=10",
            "2. Verify HPA creation:",
            "   kubectl get hpa",
            "3. Check HPA details:",
            "   kubectl describe hpa web-app"
          ]
        },
        "validations": [
          {
            "command": "kubectl get hpa web-app -o jsonpath='{.spec.minReplicas}'",
            "expected": "2",
            "points": 2,
            "description": "HPA should have minimum 2 replicas"
          },
          {
            "command": "kubectl get hpa web-app -o jsonpath='{.spec.maxReplicas}'",
            "expected": "10",
            "points": 2,
            "description": "HPA should have maximum 10 replicas"
          },
          {
            "command": "kubectl get hpa web-app -o jsonpath='{.spec.targetCPUUtilizationPercentage}'",
            "expected": "50",
            "points": 1,
            "description": "HPA should target 50% CPU utilization"
          }
        ]
      },
      {
        "id": "cka-b-020",
        "title": "Export Resource Definition",
        "description": "Export the definition of an existing deployment 'web-app' to a YAML file at /tmp/web-app-deployment.yaml without status and managed fields.",
        "points": 4,
        "timeLimit": 6,
        "category": "Workloads",
        "tags": ["export", "yaml"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["deployments"],
          "prerequisites": ["web-app deployment"]
        },
        "solution": {
          "steps": [
            "1. Export deployment to YAML:",
            "   kubectl get deployment web-app -o yaml --export > /tmp/web-app-deployment.yaml",
            "2. Alternative clean export:",
            "   kubectl get deployment web-app -o yaml | kubectl neat > /tmp/web-app-deployment.yaml",
            "3. Verify export:",
            "   head /tmp/web-app-deployment.yaml"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/web-app-deployment.yaml && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "Deployment YAML file should exist"
          },
          {
            "command": "grep -c 'kind: Deployment' /tmp/web-app-deployment.yaml",
            "expected": "1",
            "points": 2,
            "description": "File should contain deployment definition"
          }
        ]
      }
    ],
    "intermediate": [
      {
        "id": "cka-i-001",
        "title": "Backup etcd Cluster",
        "description": "Create a backup of the etcd cluster data and save it to /tmp/etcd-backup.db. Use the etcd certificates located in /etc/kubernetes/pki/etcd/.",
        "points": 8,
        "timeLimit": 15,
        "category": "Cluster Maintenance",
        "tags": ["etcd", "backup", "cluster"],
        "infrastructure": {
          "namespaces": [],
          "resources": [],
          "prerequisites": ["etcd cluster", "etcd certificates"]
        },
        "solution": {
          "steps": [
            "1. Find etcd endpoint:",
            "   kubectl get pods -n kube-system | grep etcd",
            "2. Create etcd backup:",
            "   ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \\",
            "   --endpoints=https://127.0.0.1:2379 \\",
            "   --cacert=/etc/kubernetes/pki/etcd/ca.crt \\",
            "   --cert=/etc/kubernetes/pki/etcd/server.crt \\",
            "   --key=/etc/kubernetes/pki/etcd/server.key",
            "3. Verify backup:",
            "   ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-backup.db"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/etcd-backup.db && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 4,
            "description": "etcd backup file should exist"
          },
          {
            "command": "ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-backup.db 2>/dev/null && echo 'valid' || echo 'invalid'",
            "expected": "valid",
            "points": 4,
            "description": "etcd backup should be valid"
          }
        ]
      },
      {
        "id": "cka-i-002",
        "title": "Upgrade Kubernetes Component",
        "description": "Upgrade kubelet on a worker node from the current version to the next patch version. Safely drain the node first.",
        "points": 10,
        "timeLimit": 20,
        "category": "Cluster Maintenance",
        "tags": ["upgrade", "kubelet", "maintenance"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["nodes"],
          "prerequisites": ["worker nodes", "package manager"]
        },
        "solution": {
          "steps": [
            "1. Check current kubelet version:",
            "   kubelet --version",
            "2. Drain the worker node:",
            "   kubectl drain <worker-node> --ignore-daemonsets --force",
            "3. Upgrade kubelet (on the node):",
            "   apt update && apt-mark unhold kubelet && apt install kubelet=<new-version> && apt-mark hold kubelet",
            "4. Restart kubelet:",
            "   systemctl restart kubelet",
            "5. Uncordon the node:",
            "   kubectl uncordon <worker-node>"
          ]
        },
        "validations": [
          {
            "command": "kubectl get nodes -o wide | grep -v master | head -1 | awk '{print $5}' | grep -c 'v1.28'",
            "expected": "1",
            "points": 6,
            "description": "Worker node should be running updated kubelet version"
          },
          {
            "command": "kubectl get nodes | grep -c Ready",
            "expected": "^[1-9][0-9]*$",
            "points": 4,
            "description": "All nodes should be Ready after upgrade"
          }
        ]
      },
      {
        "id": "cka-i-003",
        "title": "Troubleshoot Failing Pod",
        "description": "A pod named 'broken-app' in namespace 'troubleshoot' is failing to start. Identify the issue and fix it. The pod should use nginx:1.20 image.",
        "points": 7,
        "timeLimit": 12,
        "category": "Troubleshooting",
        "tags": ["troubleshooting", "pods"],
        "infrastructure": {
          "namespaces": ["troubleshoot"],
          "resources": ["pods"],
          "images": ["nginx:1.20"],
          "issues": ["misconfigured pod"]
        },
        "solution": {
          "steps": [
            "1. Create troubleshoot namespace:",
            "   kubectl create namespace troubleshoot",
            "2. Check pod status:",
            "   kubectl get pods -n troubleshoot",
            "3. Describe pod to find issues:",
            "   kubectl describe pod broken-app -n troubleshoot",
            "4. Check pod logs:",
            "   kubectl logs broken-app -n troubleshoot",
            "5. Fix the pod configuration and recreate"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pod broken-app -n troubleshoot -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 4,
            "description": "Pod should be running after fix"
          },
          {
            "command": "kubectl get pod broken-app -n troubleshoot -o jsonpath='{.spec.containers[0].image}'",
            "expected": "nginx:1.20",
            "points": 3,
            "description": "Pod should use correct nginx image"
          }
        ]
      },
      {
        "id": "cka-i-004",
        "title": "Configure Node Selector",
        "description": "Create a deployment 'frontend-app' with nginx:1.20 image that only schedules on nodes labeled with 'tier=frontend'. Ensure 3 replicas.",
        "points": 6,
        "timeLimit": 10,
        "category": "Scheduling",
        "tags": ["nodeSelector", "scheduling"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["deployments", "nodes"],
          "images": ["nginx:1.20"],
          "prerequisites": ["labeled nodes"]
        },
        "solution": {
          "steps": [
            "1. Create deployment with nodeSelector:",
            "   cat > frontend-deploy.yaml << EOF",
            "   apiVersion: apps/v1",
            "   kind: Deployment",
            "   metadata:",
            "     name: frontend-app",
            "   spec:",
            "     replicas: 3",
            "     selector:",
            "       matchLabels:",
            "         app: frontend-app",
            "     template:",
            "       metadata:",
            "         labels:",
            "           app: frontend-app",
            "       spec:",
            "         nodeSelector:",
            "           tier: frontend",
            "         containers:",
            "         - name: nginx",
            "           image: nginx:1.20",
            "   EOF",
            "2. Apply deployment:",
            "   kubectl apply -f frontend-deploy.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment frontend-app -o jsonpath='{.spec.replicas}'",
            "expected": "3",
            "points": 2,
            "description": "Deployment should have 3 replicas"
          },
          {
            "command": "kubectl get deployment frontend-app -o jsonpath='{.spec.template.spec.nodeSelector.tier}'",
            "expected": "frontend",
            "points": 2,
            "description": "Deployment should have nodeSelector for tier=frontend"
          },
          {
            "command": "kubectl get deployment frontend-app -o jsonpath='{.status.readyReplicas}'",
            "expected": "3",
            "points": 2,
            "description": "All replicas should be ready"
          }
        ]
      },
      {
        "id": "cka-i-005",
        "title": "Create ClusterRole and Binding",
        "description": "Create a ClusterRole 'node-reader' that can read nodes and persistentvolumes. Create a ClusterRoleBinding to bind it to user 'john'.",
        "points": 6,
        "timeLimit": 10,
        "category": "Security",
        "tags": ["rbac", "clusterrole", "security"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["clusterroles", "clusterrolebindings"]
        },
        "solution": {
          "steps": [
            "1. Create ClusterRole:",
            "   kubectl create clusterrole node-reader --verb=get,list,watch --resource=nodes,persistentvolumes",
            "2. Create ClusterRoleBinding:",
            "   kubectl create clusterrolebinding node-reader-binding --clusterrole=node-reader --user=john",
            "3. Verify creation:",
            "   kubectl describe clusterrole node-reader",
            "   kubectl describe clusterrolebinding node-reader-binding"
          ]
        },
        "validations": [
          {
            "command": "kubectl get clusterrole node-reader --no-headers | awk '{print $1}'",
            "expected": "node-reader",
            "points": 3,
            "description": "ClusterRole should exist"
          },
          {
            "command": "kubectl get clusterrolebinding node-reader-binding -o jsonpath='{.subjects[0].name}'",
            "expected": "john",
            "points": 3,
            "description": "ClusterRoleBinding should be bound to user john"
          }
        ]
      },
      {
        "id": "cka-i-006",
        "title": "Configure Resource Quotas",
        "description": "Create a ResourceQuota in namespace 'limited' that limits the namespace to 2 CPU cores, 4Gi memory, and maximum 10 pods.",
        "points": 6,
        "timeLimit": 10,
        "category": "Resource Management",
        "tags": ["resourcequota", "limits"],
        "infrastructure": {
          "namespaces": ["limited"],
          "resources": ["resourcequotas"]
        },
        "solution": {
          "steps": [
            "1. Create namespace:",
            "   kubectl create namespace limited",
            "2. Create ResourceQuota:",
            "   cat > quota.yaml << EOF",
            "   apiVersion: v1",
            "   kind: ResourceQuota",
            "   metadata:",
            "     name: compute-quota",
            "     namespace: limited",
            "   spec:",
            "     hard:",
            "       requests.cpu: \"2\"",
            "       requests.memory: 4Gi",
            "       pods: \"10\"",
            "   EOF",
            "3. Apply quota:",
            "   kubectl apply -f quota.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get resourcequota compute-quota -n limited -o jsonpath='{.spec.hard.\"requests\\.cpu\"}'",
            "expected": "2",
            "points": 2,
            "description": "ResourceQuota should limit CPU to 2 cores"
          },
          {
            "command": "kubectl get resourcequota compute-quota -n limited -o jsonpath='{.spec.hard.\"requests\\.memory\"}'",
            "expected": "4Gi",
            "points": 2,
            "description": "ResourceQuota should limit memory to 4Gi"
          },
          {
            "command": "kubectl get resourcequota compute-quota -n limited -o jsonpath='{.spec.hard.pods}'",
            "expected": "10",
            "points": 2,
            "description": "ResourceQuota should limit pods to 10"
          }
        ]
      },
      {
        "id": "cka-i-007",
        "title": "Setup Pod Security Standards",
        "description": "Configure the namespace 'secure' to enforce 'restricted' Pod Security Standards and warn on 'baseline' violations.",
        "points": 7,
        "timeLimit": 12,
        "category": "Security",
        "tags": ["security", "pss", "namespace"],
        "infrastructure": {
          "namespaces": ["secure"],
          "resources": ["namespaces"]
        },
        "solution": {
          "steps": [
            "1. Create namespace with Pod Security labels:",
            "   kubectl create namespace secure",
            "2. Label namespace for Pod Security:",
            "   kubectl label namespace secure \\",
            "     pod-security.kubernetes.io/enforce=restricted \\",
            "     pod-security.kubernetes.io/audit=restricted \\",
            "     pod-security.kubernetes.io/warn=baseline",
            "3. Verify labels:",
            "   kubectl get namespace secure --show-labels"
          ]
        },
        "validations": [
          {
            "command": "kubectl get namespace secure -o jsonpath='{.metadata.labels.\"pod-security\\.kubernetes\\.io/enforce\"}'",
            "expected": "restricted",
            "points": 3,
            "description": "Namespace should enforce restricted Pod Security Standards"
          },
          {
            "command": "kubectl get namespace secure -o jsonpath='{.metadata.labels.\"pod-security\\.kubernetes\\.io/warn\"}'",
            "expected": "baseline",
            "points": 2,
            "description": "Namespace should warn on baseline violations"
          },
          {
            "command": "kubectl get namespace secure -o jsonpath='{.metadata.labels.\"pod-security\\.kubernetes\\.io/audit\"}'",
            "expected": "restricted",
            "points": 2,
            "description": "Namespace should audit restricted violations"
          }
        ]
      },
      {
        "id": "cka-i-008",
        "title": "Configure Taints and Tolerations",
        "description": "Taint a worker node with 'app=database:NoSchedule' and create a pod 'db-pod' with nginx image that tolerates this taint.",
        "points": 7,
        "timeLimit": 12,
        "category": "Scheduling",
        "tags": ["taints", "tolerations", "scheduling"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["nodes", "pods"],
          "images": ["nginx"],
          "prerequisites": ["worker nodes"]
        },
        "solution": {
          "steps": [
            "1. Get worker nodes:",
            "   kubectl get nodes",
            "2. Taint a worker node:",
            "   kubectl taint nodes <worker-node-name> app=database:NoSchedule",
            "3. Create pod with toleration:",
            "   cat > db-pod.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Pod",
            "   metadata:",
            "     name: db-pod",
            "   spec:",
            "     tolerations:",
            "     - key: \"app\"",
            "       operator: \"Equal\"",
            "       value: \"database\"",
            "       effect: \"NoSchedule\"",
            "     containers:",
            "     - name: nginx",
            "       image: nginx",
            "   EOF",
            "4. Apply pod:",
            "   kubectl apply -f db-pod.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get nodes -o json | jq -r '.items[] | select(.spec.taints != null) | .spec.taints[] | select(.key == \"app\" and .value == \"database\") | .effect'",
            "expected": "NoSchedule",
            "points": 3,
            "description": "Node should have app=database:NoSchedule taint"
          },
          {
            "command": "kubectl get pod db-pod -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 2,
            "description": "Pod should be running despite taint"
          },
          {
            "command": "kubectl get pod db-pod -o jsonpath='{.spec.tolerations[0].key}'",
            "expected": "app",
            "points": 2,
            "description": "Pod should have toleration for app key"
          }
        ]
      },
      {
        "id": "cka-i-009",
        "title": "Monitor Cluster Components",
        "description": "Check the health of all cluster components (API server, etcd, scheduler, controller-manager) and save the status to /tmp/component-status.txt.",
        "points": 6,
        "timeLimit": 10,
        "category": "Troubleshooting",
        "tags": ["monitoring", "components"],
        "infrastructure": {
          "namespaces": ["kube-system"],
          "resources": ["pods", "componentstatuses"]
        },
        "solution": {
          "steps": [
            "1. Check component status:",
            "   kubectl get componentstatuses",
            "2. Check system pods:",
            "   kubectl get pods -n kube-system",
            "3. Save component status:",
            "   kubectl get componentstatuses > /tmp/component-status.txt",
            "4. Append system pods status:",
            "   kubectl get pods -n kube-system >> /tmp/component-status.txt"
          ]
        },
        "validations": [
          {
            "command": "test -f /tmp/component-status.txt && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 2,
            "description": "Component status file should exist"
          },
          {
            "command": "grep -c 'Healthy\\|Running' /tmp/component-status.txt",
            "expected": "^[1-9][0-9]*$",
            "points": 4,
            "description": "File should contain healthy/running components"
          }
        ]
      },
      {
        "id": "cka-i-010",
        "title": "Create StatefulSet",
        "description": "Create a StatefulSet named 'web-stateful' with nginx:1.20 image, 3 replicas, and a headless service named 'web-service'.",
        "points": 8,
        "timeLimit": 15,
        "category": "Workloads",
        "tags": ["statefulset", "headless service"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["statefulsets", "services"],
          "images": ["nginx:1.20"]
        },
        "solution": {
          "steps": [
            "1. Create headless service:",
            "   cat > web-service.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Service",
            "   metadata:",
            "     name: web-service",
            "   spec:",
            "     clusterIP: None",
            "     selector:",
            "       app: web-stateful",
            "     ports:",
            "     - port: 80",
            "   EOF",
            "2. Create StatefulSet:",
            "   cat > statefulset.yaml << EOF",
            "   apiVersion: apps/v1",
            "   kind: StatefulSet",
            "   metadata:",
            "     name: web-stateful",
            "   spec:",
            "     serviceName: web-service",
            "     replicas: 3",
            "     selector:",
            "       matchLabels:",
            "         app: web-stateful",
            "     template:",
            "       metadata:",
            "         labels:",
            "           app: web-stateful",
            "       spec:",
            "         containers:",
            "         - name: nginx",
            "           image: nginx:1.20",
            "   EOF",
            "3. Apply manifests:",
            "   kubectl apply -f web-service.yaml -f statefulset.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get statefulset web-stateful -o jsonpath='{.spec.replicas}'",
            "expected": "3",
            "points": 2,
            "description": "StatefulSet should have 3 replicas"
          },
          {
            "command": "kubectl get service web-service -o jsonpath='{.spec.clusterIP}'",
            "expected": "None",
            "points": 3,
            "description": "Service should be headless (clusterIP: None)"
          },
          {
            "command": "kubectl get statefulset web-stateful -o jsonpath='{.status.readyReplicas}'",
            "expected": "3",
            "points": 3,
            "description": "All StatefulSet replicas should be ready"
          }
        ]
      },
      {
        "id": "cka-i-011",
        "title": "Configure Pod Disruption Budget",
        "description": "Create a PodDisruptionBudget for the 'web-app' deployment that ensures at least 2 pods are always available during disruptions.",
        "points": 6,
        "timeLimit": 10,
        "category": "Workloads",
        "tags": ["pdb", "availability"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["poddisruptionbudgets", "deployments"],
          "prerequisites": ["web-app deployment"]
        },
        "solution": {
          "steps": [
            "1. Create PodDisruptionBudget:",
            "   cat > pdb.yaml << EOF",
            "   apiVersion: policy/v1",
            "   kind: PodDisruptionBudget",
            "   metadata:",
            "     name: web-app-pdb",
            "   spec:",
            "     minAvailable: 2",
            "     selector:",
            "       matchLabels:",
            "         app: web-app",
            "   EOF",
            "2. Apply PDB:",
            "   kubectl apply -f pdb.yaml",
            "3. Verify PDB:",
            "   kubectl get pdb"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pdb web-app-pdb -o jsonpath='{.spec.minAvailable}'",
            "expected": "2",
            "points": 3,
            "description": "PDB should ensure minimum 2 pods available"
          },
          {
            "command": "kubectl get pdb web-app-pdb -o jsonpath='{.spec.selector.matchLabels.app}'",
            "expected": "web-app",
            "points": 3,
            "description": "PDB should target web-app pods"
          }
        ]
      },
      {
        "id": "cka-i-012",
        "title": "Configure Custom Scheduler",
        "description": "Deploy a custom scheduler named 'my-scheduler' and create a pod that uses this custom scheduler.",
        "points": 9,
        "timeLimit": 18,
        "category": "Scheduling",
        "tags": ["scheduler", "custom"],
        "infrastructure": {
          "namespaces": ["kube-system", "default"],
          "resources": ["deployments", "pods"],
          "images": ["k8s.gcr.io/kube-scheduler"]
        },
        "solution": {
          "steps": [
            "1. Create custom scheduler deployment:",
            "   cat > custom-scheduler.yaml << EOF",
            "   apiVersion: apps/v1",
            "   kind: Deployment",
            "   metadata:",
            "     name: my-scheduler",
            "     namespace: kube-system",
            "   spec:",
            "     replicas: 1",
            "     selector:",
            "       matchLabels:",
            "         app: my-scheduler",
            "     template:",
            "       metadata:",
            "         labels:",
            "           app: my-scheduler",
            "       spec:",
            "         containers:",
            "         - name: kube-scheduler",
            "           image: k8s.gcr.io/kube-scheduler:v1.28.0",
            "           command:",
            "           - kube-scheduler",
            "           - --config=/etc/kubernetes/my-scheduler-config.yaml",
            "           - --v=2",
            "   EOF",
            "2. Create pod using custom scheduler:",
            "   cat > scheduled-pod.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Pod",
            "   metadata:",
            "     name: custom-scheduled-pod",
            "   spec:",
            "     schedulerName: my-scheduler",
            "     containers:",
            "     - name: nginx",
            "       image: nginx",
            "   EOF",
            "3. Apply manifests:",
            "   kubectl apply -f custom-scheduler.yaml -f scheduled-pod.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment my-scheduler -n kube-system --no-headers | awk '{print $1}'",
            "expected": "my-scheduler",
            "points": 4,
            "description": "Custom scheduler deployment should exist"
          },
          {
            "command": "kubectl get pod custom-scheduled-pod -o jsonpath='{.spec.schedulerName}'",
            "expected": "my-scheduler",
            "points": 3,
            "description": "Pod should use custom scheduler"
          },
          {
            "command": "kubectl get pod custom-scheduled-pod -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 2,
            "description": "Pod should be successfully scheduled and running"
          }
        ]
      },
      {
        "id": "cka-i-013",
        "title": "Restore etcd from Backup",
        "description": "Restore the etcd cluster from the backup created at /tmp/etcd-backup.db to a new data directory /var/lib/etcd-restore.",
        "points": 9,
        "timeLimit": 18,
        "category": "Cluster Maintenance",
        "tags": ["etcd", "restore", "backup"],
        "infrastructure": {
          "namespaces": [],
          "resources": [],
          "prerequisites": ["etcd backup", "etcd cluster"],
          "directories": ["/var/lib/etcd-restore"]
        },
        "solution": {
          "steps": [
            "1. Stop etcd (if running as systemd service):",
            "   systemctl stop etcd",
            "2. Restore from backup:",
            "   ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \\",
            "   --data-dir=/var/lib/etcd-restore \\",
            "   --name=etcd-restored \\",
            "   --initial-cluster=etcd-restored=https://127.0.0.1:2380 \\",
            "   --initial-advertise-peer-urls=https://127.0.0.1:2380",
            "3. Update etcd configuration to use new data directory:",
            "   # Edit /etc/kubernetes/manifests/etcd.yaml",
            "4. Restart etcd or kubelet to reload configuration"
          ]
        },
        "validations": [
          {
            "command": "test -d /var/lib/etcd-restore && echo 'exists' || echo 'missing'",
            "expected": "exists",
            "points": 4,
            "description": "Restored etcd data directory should exist"
          },
          {
            "command": "ls -la /var/lib/etcd-restore | grep -c member",
            "expected": "1",
            "points": 3,
            "description": "Restored directory should contain member data"
          },
          {
            "command": "kubectl get nodes --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 2,
            "description": "Cluster should be functional after restore"
          }
        ]
      },
      {
        "id": "cka-i-014",
        "title": "Configure Ingress Controller",
        "description": "Deploy an nginx ingress controller and create an Ingress resource for the 'web-app' service on path '/app'.",
        "points": 8,
        "timeLimit": 15,
        "category": "Networking",
        "tags": ["ingress", "nginx", "networking"],
        "infrastructure": {
          "namespaces": ["ingress-nginx", "default"],
          "resources": ["deployments", "services", "ingress"],
          "prerequisites": ["web-app service"]
        },
        "solution": {
          "steps": [
            "1. Deploy nginx ingress controller:",
            "   kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml",
            "2. Wait for ingress controller:",
            "   kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=controller -n ingress-nginx",
            "3. Create ingress resource:",
            "   cat > ingress.yaml << EOF",
            "   apiVersion: networking.k8s.io/v1",
            "   kind: Ingress",
            "   metadata:",
            "     name: web-app-ingress",
            "   spec:",
            "     ingressClassName: nginx",
            "     rules:",
            "     - http:",
            "         paths:",
            "         - path: /app",
            "           pathType: Prefix",
            "           backend:",
            "             service:",
            "               name: web-app",
            "               port:",
            "                 number: 80",
            "   EOF",
            "4. Apply ingress:",
            "   kubectl apply -f ingress.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment ingress-nginx-controller -n ingress-nginx --no-headers | awk '{print $1}'",
            "expected": "ingress-nginx-controller",
            "points": 4,
            "description": "Nginx ingress controller should be deployed"
          },
          {
            "command": "kubectl get ingress web-app-ingress -o jsonpath='{.spec.rules[0].http.paths[0].path}'",
            "expected": "/app",
            "points": 2,
            "description": "Ingress should have /app path"
          },
          {
            "command": "kubectl get ingress web-app-ingress -o jsonpath='{.spec.rules[0].http.paths[0].backend.service.name}'",
            "expected": "web-app",
            "points": 2,
            "description": "Ingress should route to web-app service"
          }
        ]
      },
      {
        "id": "cka-i-015",
        "title": "Configure Pod Security Context",
        "description": "Create a pod 'secure-pod' with busybox:1.35 image that runs as non-root user (UID 1000), read-only root filesystem, and drops all capabilities.",
        "points": 7,
        "timeLimit": 12,
        "category": "Security",
        "tags": ["security", "securitycontext"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["pods"],
          "images": ["busybox:1.35"]
        },
        "solution": {
          "steps": [
            "1. Create pod with security context:",
            "   cat > secure-pod.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Pod",
            "   metadata:",
            "     name: secure-pod",
            "   spec:",
            "     securityContext:",
            "       runAsUser: 1000",
            "       runAsNonRoot: true",
            "       fsGroup: 1000",
            "     containers:",
            "     - name: busybox",
            "       image: busybox:1.35",
            "       command: ['sleep', '3600']",
            "       securityContext:",
            "         readOnlyRootFilesystem: true",
            "         allowPrivilegeEscalation: false",
            "         capabilities:",
            "           drop:",
            "           - ALL",
            "   EOF",
            "2. Apply pod:",
            "   kubectl apply -f secure-pod.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pod secure-pod -o jsonpath='{.spec.securityContext.runAsUser}'",
            "expected": "1000",
            "points": 2,
            "description": "Pod should run as user 1000"
          },
          {
            "command": "kubectl get pod secure-pod -o jsonpath='{.spec.containers[0].securityContext.readOnlyRootFilesystem}'",
            "expected": "true",
            "points": 2,
            "description": "Pod should have read-only root filesystem"
          },
          {
            "command": "kubectl get pod secure-pod -o jsonpath='{.spec.containers[0].securityContext.capabilities.drop[0]}'",
            "expected": "ALL",
            "points": 2,
            "description": "Pod should drop all capabilities"
          },
          {
            "command": "kubectl get pod secure-pod -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 1,
            "description": "Pod should be running"
          }
        ]
      },
      {
        "id": "cka-i-016",
        "title": "Configure DaemonSet",
        "description": "Create a DaemonSet named 'log-collector' using busybox:1.35 image that runs on all worker nodes to collect logs.",
        "points": 6,
        "timeLimit": 10,
        "category": "Workloads",
        "tags": ["daemonset", "workloads"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["daemonsets"],
          "images": ["busybox:1.35"]
        },
        "solution": {
          "steps": [
            "1. Create DaemonSet:",
            "   cat > daemonset.yaml << EOF",
            "   apiVersion: apps/v1",
            "   kind: DaemonSet",
            "   metadata:",
            "     name: log-collector",
            "   spec:",
            "     selector:",
            "       matchLabels:",
            "         app: log-collector",
            "     template:",
            "       metadata:",
            "         labels:",
            "           app: log-collector",
            "       spec:",
            "         containers:",
            "         - name: log-collector",
            "           image: busybox:1.35",
            "           command: ['sh', '-c', 'while true; do echo Collecting logs; sleep 30; done']",
            "   EOF",
            "2. Apply DaemonSet:",
            "   kubectl apply -f daemonset.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get daemonset log-collector --no-headers | awk '{print $1}'",
            "expected": "log-collector",
            "points": 3,
            "description": "DaemonSet should exist"
          },
          {
            "command": "kubectl get daemonset log-collector -o jsonpath='{.status.numberReady}'",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "DaemonSet should have ready pods on worker nodes"
          }
        ]
      },
      {
        "id": "cka-i-017",
        "title": "Configure Resource Limits",
        "description": "Create a pod 'limited-pod' with nginx:1.20 image that has CPU request of 100m, CPU limit of 200m, memory request of 128Mi, and memory limit of 256Mi.",
        "points": 6,
        "timeLimit": 10,
        "category": "Resource Management",
        "tags": ["resources", "limits"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["pods"],
          "images": ["nginx:1.20"]
        },
        "solution": {
          "steps": [
            "1. Create pod with resource limits:",
            "   cat > limited-pod.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Pod",
            "   metadata:",
            "     name: limited-pod",
            "   spec:",
            "     containers:",
            "     - name: nginx",
            "       image: nginx:1.20",
            "       resources:",
            "         requests:",
            "           cpu: 100m",
            "           memory: 128Mi",
            "         limits:",
            "           cpu: 200m",
            "           memory: 256Mi",
            "   EOF",
            "2. Apply pod:",
            "   kubectl apply -f limited-pod.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pod limited-pod -o jsonpath='{.spec.containers[0].resources.requests.cpu}'",
            "expected": "100m",
            "points": 2,
            "description": "Pod should have 100m CPU request"
          },
          {
            "command": "kubectl get pod limited-pod -o jsonpath='{.spec.containers[0].resources.limits.memory}'",
            "expected": "256Mi",
            "points": 2,
            "description": "Pod should have 256Mi memory limit"
          },
          {
            "command": "kubectl get pod limited-pod -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 2,
            "description": "Pod should be running"
          }
        ]
      },
      {
        "id": "cka-i-018",
        "title": "Configure Persistent Volume Claim",
        "description": "Create a PersistentVolumeClaim named 'storage-claim' requesting 500Mi of storage with ReadWriteOnce access mode, then create a pod that uses it.",
        "points": 7,
        "timeLimit": 12,
        "category": "Storage",
        "tags": ["pvc", "storage"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["persistentvolumeclaims", "pods"],
          "prerequisites": ["available PV"]
        },
        "solution": {
          "steps": [
            "1. Create PVC:",
            "   cat > pvc.yaml << EOF",
            "   apiVersion: v1",
            "   kind: PersistentVolumeClaim",
            "   metadata:",
            "     name: storage-claim",
            "   spec:",
            "     accessModes:",
            "       - ReadWriteOnce",
            "     resources:",
            "       requests:",
            "         storage: 500Mi",
            "   EOF",
            "2. Create pod using PVC:",
            "   cat > pod-with-pvc.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Pod",
            "   metadata:",
            "     name: storage-pod",
            "   spec:",
            "     containers:",
            "     - name: nginx",
            "       image: nginx",
            "       volumeMounts:",
            "       - name: storage",
            "         mountPath: /data",
            "     volumes:",
            "     - name: storage",
            "       persistentVolumeClaim:",
            "         claimName: storage-claim",
            "   EOF",
            "3. Apply manifests:",
            "   kubectl apply -f pvc.yaml -f pod-with-pvc.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pvc storage-claim -o jsonpath='{.spec.resources.requests.storage}'",
            "expected": "500Mi",
            "points": 3,
            "description": "PVC should request 500Mi storage"
          },
          {
            "command": "kubectl get pvc storage-claim -o jsonpath='{.status.phase}'",
            "expected": "Bound",
            "points": 2,
            "description": "PVC should be bound"
          },
          {
            "command": "kubectl get pod storage-pod -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 2,
            "description": "Pod using PVC should be running"
          }
        ]
      },
      {
        "id": "cka-i-019",
        "title": "Configure CPU Node Affinity",
        "description": "Create a deployment 'cpu-intensive' with nginx:1.20 image that prefers nodes with label 'cpu=high-performance' but can schedule elsewhere.",
        "points": 7,
        "timeLimit": 12,
        "category": "Scheduling",
        "tags": ["affinity", "scheduling"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["deployments"],
          "images": ["nginx:1.20"],
          "prerequisites": ["labeled nodes"]
        },
        "solution": {
          "steps": [
            "1. Label a node (optional):",
            "   kubectl label node <node-name> cpu=high-performance",
            "2. Create deployment with node affinity:",
            "   cat > cpu-intensive.yaml << EOF",
            "   apiVersion: apps/v1",
            "   kind: Deployment",
            "   metadata:",
            "     name: cpu-intensive",
            "   spec:",
            "     replicas: 2",
            "     selector:",
            "       matchLabels:",
            "         app: cpu-intensive",
            "     template:",
            "       metadata:",
            "         labels:",
            "           app: cpu-intensive",
            "       spec:",
            "         affinity:",
            "           nodeAffinity:",
            "             preferredDuringSchedulingIgnoredDuringExecution:",
            "             - weight: 1",
            "               preference:",
            "                 matchExpressions:",
            "                 - key: cpu",
            "                   operator: In",
            "                   values:",
            "                   - high-performance",
            "         containers:",
            "         - name: nginx",
            "           image: nginx:1.20",
            "   EOF",
            "3. Apply deployment:",
            "   kubectl apply -f cpu-intensive.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment cpu-intensive -o jsonpath='{.spec.template.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[0].preference.matchExpressions[0].key}'",
            "expected": "cpu",
            "points": 3,
            "description": "Deployment should have node affinity for cpu label"
          },
          {
            "command": "kubectl get deployment cpu-intensive -o jsonpath='{.spec.template.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[0].preference.matchExpressions[0].values[0]}'",
            "expected": "high-performance",
            "points": 2,
            "description": "Node affinity should prefer high-performance value"
          },
          {
            "command": "kubectl get deployment cpu-intensive -o jsonpath='{.status.readyReplicas}'",
            "expected": "2",
            "points": 2,
            "description": "Deployment should have 2 ready replicas"
          }
        ]
      },
      {
        "id": "cka-i-020",
        "title": "Configure Multi-Container Pod",
        "description": "Create a pod 'multi-container' with two containers: nginx:1.20 serving on port 80 and busybox:1.35 running a sidecar that logs requests.",
        "points": 7,
        "timeLimit": 12,
        "category": "Workloads",
        "tags": ["multi-container", "sidecar"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["pods"],
          "images": ["nginx:1.20", "busybox:1.35"]
        },
        "solution": {
          "steps": [
            "1. Create multi-container pod:",
            "   cat > multi-container.yaml << EOF",
            "   apiVersion: v1",
            "   kind: Pod",
            "   metadata:",
            "     name: multi-container",
            "   spec:",
            "     containers:",
            "     - name: nginx",
            "       image: nginx:1.20",
            "       ports:",
            "       - containerPort: 80",
            "       volumeMounts:",
            "       - name: shared-logs",
            "         mountPath: /var/log/nginx",
            "     - name: log-sidecar",
            "       image: busybox:1.35",
            "       command: ['sh', '-c', 'while true; do echo \"$(date): Sidecar logging\"; sleep 10; done']",
            "       volumeMounts:",
            "       - name: shared-logs",
            "         mountPath: /logs",
            "     volumes:",
            "     - name: shared-logs",
            "       emptyDir: {}",
            "   EOF",
            "2. Apply pod:",
            "   kubectl apply -f multi-container.yaml"
          ]
        },
        "validations": [
          {
            "command": "kubectl get pod multi-container -o jsonpath='{.spec.containers[0].name}'",
            "expected": "nginx",
            "points": 2,
            "description": "Pod should have nginx container"
          },
          {
            "command": "kubectl get pod multi-container -o jsonpath='{.spec.containers[1].name}'",
            "expected": "log-sidecar",
            "points": 2,
            "description": "Pod should have sidecar container"
          },
          {
            "command": "kubectl get pod multi-container -o jsonpath='{.status.phase}'",
            "expected": "Running",
            "points": 2,
            "description": "Multi-container pod should be running"
          },
          {
            "command": "kubectl get pod multi-container -o jsonpath='{.status.containerStatuses[*].ready}' | grep -c true",
            "expected": "2",
            "points": 1,
            "description": "Both containers should be ready"
          }
        ]
      }
    ],
    "advanced": [
      {
        "id": "cka-a-001",
        "title": "Setup High Availability etcd Cluster",
        "description": "Configure a 3-node etcd cluster for high availability. Include proper TLS certificates and member configuration.",
        "points": 15,
        "timeLimit": 30,
        "category": "Cluster Architecture",
        "tags": ["etcd", "ha", "tls"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["etcd cluster"],
          "prerequisites": ["3 nodes", "TLS certificates"]
        },
        "solution": {
          "steps": [
            "1. Generate TLS certificates for etcd cluster",
            "2. Configure etcd on each node with proper endpoints",
            "3. Start etcd cluster with proper member configuration",
            "4. Verify cluster health and member status",
            "5. Test cluster functionality"
          ]
        },
        "validations": [
          {
            "command": "ETCDCTL_API=3 etcdctl member list --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key | wc -l",
            "expected": "3",
            "points": 8,
            "description": "etcd cluster should have 3 members"
          },
          {
            "command": "ETCDCTL_API=3 etcdctl endpoint health --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key | grep -c 'is healthy'",
            "expected": "1",
            "points": 7,
            "description": "etcd endpoints should be healthy"
          }
        ]
      },
      {
        "id": "cka-a-002",
        "title": "Implement Custom Resource Definition",
        "description": "Create a CRD for 'Database' resources with spec fields for engine, version, and replicas. Create a custom controller to manage these resources.",
        "points": 12,
        "timeLimit": 25,
        "category": "Cluster Architecture",
        "tags": ["crd", "controller", "custom"],
        "infrastructure": {
          "namespaces": ["default"],
          "resources": ["customresourcedefinitions", "deployments"],
          "prerequisites": ["operator framework"]
        },
        "solution": {
          "steps": [
            "1. Create CRD for Database resource",
            "2. Apply the CRD to cluster",
            "3. Create a simple controller deployment",
            "4. Test by creating Database custom resources",
            "5. Verify controller functionality"
          ]
        },
        "validations": [
          {
            "command": "kubectl get crd databases.example.com --no-headers | awk '{print $1}'",
            "expected": "databases.example.com",
            "points": 6,
            "description": "Database CRD should exist"
          },
          {
            "command": "kubectl get databases --no-headers 2>/dev/null | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 6,
            "description": "Should be able to create Database resources"
          }
        ]
      },
      {
        "id": "cka-a-003",
        "title": "Configure Advanced RBAC",
        "description": "Create a complex RBAC setup with multiple roles, groups, and service accounts. Include namespace-specific permissions and cluster-wide read access.",
        "points": 10,
        "timeLimit": 20,
        "category": "Security",
        "tags": ["rbac", "advanced", "security"],
        "infrastructure": {
          "namespaces": ["dev", "prod", "monitoring"],
          "resources": ["roles", "clusterroles", "rolebindings", "clusterrolebindings", "serviceaccounts"]
        },
        "solution": {
          "steps": [
            "1. Create namespaces: dev, prod, monitoring",
            "2. Create service accounts for different teams",
            "3. Create namespace-specific roles",
            "4. Create cluster-wide monitoring role",
            "5. Bind roles to appropriate subjects",
            "6. Test permissions with kubectl auth can-i"
          ]
        },
        "validations": [
          {
            "command": "kubectl get clusterrole monitoring-reader --no-headers | awk '{print $1}'",
            "expected": "monitoring-reader",
            "points": 3,
            "description": "Monitoring ClusterRole should exist"
          },
          {
            "command": "kubectl get role dev-admin -n dev --no-headers | awk '{print $1}'",
            "expected": "dev-admin",
            "points": 3,
            "description": "Dev admin role should exist in dev namespace"
          },
          {
            "command": "kubectl get serviceaccount dev-user -n dev --no-headers | awk '{print $1}'",
            "expected": "dev-user",
            "points": 2,
            "description": "Dev user service account should exist"
          },
          {
            "command": "kubectl auth can-i list pods --as=system:serviceaccount:dev:dev-user -n dev && echo 'allowed' || echo 'denied'",
            "expected": "allowed",
            "points": 2,
            "description": "Dev user should be able to list pods in dev namespace"
          }
        ]
      },
      {
        "id": "cka-a-004",
        "title": "Implement Pod Security Admission",
        "description": "Configure Pod Security Admission with custom policies. Create enforcement, audit, and warn configurations across different namespaces.",
        "points": 10,
        "timeLimit": 20,
        "category": "Security",
        "tags": ["security", "admission", "policy"],
        "infrastructure": {
          "namespaces": ["restricted", "baseline", "privileged"],
          "resources": ["pods", "deployments"],
          "prerequisites": ["Pod Security Admission enabled"]
        },
        "solution": {
          "steps": [
            "1. Create namespaces with different security levels",
            "2. Configure Pod Security labels on namespaces",
            "3. Test with pods of different security contexts",
            "4. Verify enforcement, audit, and warn behaviors",
            "5. Create documentation of security policies"
          ]
        },
        "validations": [
          {
            "command": "kubectl get namespace restricted -o jsonpath='{.metadata.labels.\"pod-security\\.kubernetes\\.io/enforce\"}'",
            "expected": "restricted",
            "points": 3,
            "description": "Restricted namespace should enforce restricted policy"
          },
          {
            "command": "kubectl get namespace baseline -o jsonpath='{.metadata.labels.\"pod-security\\.kubernetes\\.io/enforce\"}'",
            "expected": "baseline",
            "points": 3,
            "description": "Baseline namespace should enforce baseline policy"
          },
          {
            "command": "kubectl get namespace privileged -o jsonpath='{.metadata.labels.\"pod-security\\.kubernetes\\.io/enforce\"}'",
            "expected": "privileged",
            "points": 2,
            "description": "Privileged namespace should enforce privileged policy"
          },
          {
            "command": "kubectl run test-pod --image=nginx --dry-run=server -n restricted 2>&1 | grep -c 'violates.*restricted' || echo '0'",
            "expected": "^[1-9][0-9]*$",
            "points": 2,
            "description": "Restricted namespace should block non-compliant pods"
          }
        ]
      },
      {
        "id": "cka-a-005",
        "title": "Configure Advanced Networking",
        "description": "Set up complex network policies with ingress/egress rules, multiple namespaces, and port-specific access controls.",
        "points": 12,
        "timeLimit": 25,
        "category": "Networking",
        "tags": ["networking", "policies", "security"],
        "infrastructure": {
          "namespaces": ["frontend", "backend", "database"],
          "resources": ["networkpolicies", "pods", "services"],
          "prerequisites": ["CNI with NetworkPolicy support"]
        },
        "solution": {
          "steps": [
            "1. Create frontend, backend, database namespaces",
            "2. Deploy test applications in each namespace",
            "3. Create network policies for tier-based access",
            "4. Configure ingress policies for frontend",
            "5. Configure egress policies for backend",
            "6. Test connectivity between tiers",
            "7. Verify isolation is working"
          ]
        },
        "validations": [
          {
            "command": "kubectl get networkpolicy -n frontend --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Frontend namespace should have network policies"
          },
          {
            "command": "kubectl get networkpolicy -n backend --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Backend namespace should have network policies"
          },
          {
            "command": "kubectl get networkpolicy -n database --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Database namespace should have network policies"
          },
          {
            "command": "kubectl get pods -n frontend --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Frontend pods should be running for testing"
          }
        ]
      },
      {
        "id": "cka-a-006",
        "title": "Implement Cluster Autoscaling",
        "description": "Configure cluster autoscaler to automatically scale nodes based on pod resource requirements. Include node groups and scaling policies.",
        "points": 12,
        "timeLimit": 25,
        "category": "Cluster Maintenance",
        "tags": ["autoscaling", "nodes", "cluster"],
        "infrastructure": {
          "namespaces": ["kube-system"],
          "resources": ["deployments", "configmaps"],
          "prerequisites": ["cloud provider", "node groups"]
        },
        "solution": {
          "steps": [
            "1. Deploy cluster autoscaler",
            "2. Configure autoscaler with proper cloud provider settings",
            "3. Set up node group configurations",
            "4. Configure scaling policies and limits",
            "5. Test autoscaling with resource-intensive workloads",
            "6. Verify scale-up and scale-down behaviors"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment cluster-autoscaler -n kube-system --no-headers | awk '{print $1}'",
            "expected": "cluster-autoscaler",
            "points": 6,
            "description": "Cluster autoscaler should be deployed"
          },
          {
            "command": "kubectl get configmap cluster-autoscaler-status -n kube-system --no-headers | awk '{print $1}'",
            "expected": "cluster-autoscaler-status",
            "points": 3,
            "description": "Autoscaler status configmap should exist"
          },
          {
            "command": "kubectl logs -n kube-system deployment/cluster-autoscaler | grep -c 'Starting cluster autoscaler' || echo '0'",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Cluster autoscaler should be running"
          }
        ]
      },
      {
        "id": "cka-a-007",
        "title": "Configure Advanced Storage Classes",
        "description": "Create multiple storage classes with different provisioners, reclaim policies, and volume binding modes. Implement storage quotas and limits.",
        "points": 10,
        "timeLimit": 20,
        "category": "Storage",
        "tags": ["storage", "provisioners", "quotas"],
        "infrastructure": {
          "namespaces": ["storage-test"],
          "resources": ["storageclasses", "persistentvolumes", "persistentvolumeclaims"],
          "prerequisites": ["storage provisioners"]
        },
        "solution": {
          "steps": [
            "1. Create storage classes with different provisioners",
            "2. Configure reclaim policies (Retain, Delete)",
            "3. Set up volume binding modes (Immediate, WaitForFirstConsumer)",
            "4. Create storage quotas for namespace",
            "5. Test storage provisioning with different classes",
            "6. Verify storage quotas are enforced"
          ]
        },
        "validations": [
          {
            "command": "kubectl get storageclass fast-ssd --no-headers | awk '{print $1}'",
            "expected": "fast-ssd",
            "points": 3,
            "description": "Fast SSD storage class should exist"
          },
          {
            "command": "kubectl get storageclass slow-hdd --no-headers | awk '{print $1}'",
            "expected": "slow-hdd",
            "points": 3,
            "description": "Slow HDD storage class should exist"
          },
          {
            "command": "kubectl get storageclass fast-ssd -o jsonpath='{.reclaimPolicy}'",
            "expected": "Retain",
            "points": 2,
            "description": "Fast SSD should have Retain reclaim policy"
          },
          {
            "command": "kubectl get resourcequota storage-quota -n storage-test --no-headers | awk '{print $1}'",
            "expected": "storage-quota",
            "points": 2,
            "description": "Storage quota should exist in test namespace"
          }
        ]
      },
      {
        "id": "cka-a-008",
        "title": "Implement Advanced Monitoring",
        "description": "Deploy Prometheus, Grafana, and AlertManager. Configure custom metrics, alerts, and dashboards for cluster monitoring.",
        "points": 15,
        "timeLimit": 30,
        "category": "Monitoring",
        "tags": ["monitoring", "prometheus", "grafana"],
        "infrastructure": {
          "namespaces": ["monitoring"],
          "resources": ["deployments", "services", "configmaps", "secrets"],
          "prerequisites": ["persistent storage"]
        },
        "solution": {
          "steps": [
            "1. Deploy Prometheus with persistent storage",
            "2. Configure Prometheus scraping for cluster components",
            "3. Deploy Grafana with data source configuration",
            "4. Set up AlertManager with notification channels",
            "5. Create custom metrics and alerts",
            "6. Import and configure monitoring dashboards",
            "7. Test alert firing and resolution"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment prometheus -n monitoring --no-headers | awk '{print $1}'",
            "expected": "prometheus",
            "points": 5,
            "description": "Prometheus should be deployed"
          },
          {
            "command": "kubectl get deployment grafana -n monitoring --no-headers | awk '{print $1}'",
            "expected": "grafana",
            "points": 4,
            "description": "Grafana should be deployed"
          },
          {
            "command": "kubectl get deployment alertmanager -n monitoring --no-headers | awk '{print $1}'",
            "expected": "alertmanager",
            "points": 3,
            "description": "AlertManager should be deployed"
          },
          {
            "command": "kubectl get service prometheus -n monitoring --no-headers | awk '{print $1}'",
            "expected": "prometheus",
            "points": 2,
            "description": "Prometheus service should be accessible"
          },
          {
            "command": "kubectl get configmap prometheus-config -n monitoring --no-headers | awk '{print $1}'",
            "expected": "prometheus-config",
            "points": 1,
            "description": "Prometheus configuration should exist"
          }
        ]
      },
      {
        "id": "cka-a-009",
        "title": "Configure Multi-Cluster Federation",
        "description": "Set up federation between multiple Kubernetes clusters. Configure cross-cluster service discovery and workload distribution.",
        "points": 15,
        "timeLimit": 30,
        "category": "Cluster Architecture",
        "tags": ["federation", "multi-cluster", "distribution"],
        "infrastructure": {
          "namespaces": ["federation-system"],
          "resources": ["deployments", "services", "configmaps"],
          "prerequisites": ["multiple clusters", "cluster connectivity"]
        },
        "solution": {
          "steps": [
            "1. Install federation control plane",
            "2. Join multiple clusters to federation",
            "3. Configure cross-cluster networking",
            "4. Set up federated services and deployments",
            "5. Configure load balancing across clusters",
            "6. Test cross-cluster service discovery",
            "7. Verify workload distribution"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment federation-controller -n federation-system --no-headers | awk '{print $1}'",
            "expected": "federation-controller",
            "points": 8,
            "description": "Federation controller should be deployed"
          },
          {
            "command": "kubectl get clusters --no-headers 2>/dev/null | wc -l",
            "expected": "^[2-9][0-9]*$",
            "points": 7,
            "description": "Multiple clusters should be federated"
          }
        ]
      },
      {
        "id": "cka-a-010",
        "title": "Implement Advanced Backup Strategy",
        "description": "Create comprehensive backup strategy including etcd, persistent volumes, and application data. Implement automated backup scheduling and retention.",
        "points": 12,
        "timeLimit": 25,
        "category": "Cluster Maintenance",
        "tags": ["backup", "automation", "retention"],
        "infrastructure": {
          "namespaces": ["backup-system"],
          "resources": ["cronjobs", "persistentvolumes", "secrets"],
          "prerequisites": ["backup storage", "etcd access"]
        },
        "solution": {
          "steps": [
            "1. Create backup storage configuration",
            "2. Set up automated etcd backup jobs",
            "3. Configure persistent volume backup",
            "4. Implement application data backup",
            "5. Create backup retention policies",
            "6. Set up backup monitoring and alerting",
            "7. Test backup and restore procedures"
          ]
        },
        "validations": [
          {
            "command": "kubectl get cronjob etcd-backup -n backup-system --no-headers | awk '{print $1}'",
            "expected": "etcd-backup",
            "points": 4,
            "description": "Automated etcd backup job should exist"
          },
          {
            "command": "kubectl get cronjob pv-backup -n backup-system --no-headers | awk '{print $1}'",
            "expected": "pv-backup",
            "points": 4,
            "description": "Persistent volume backup job should exist"
          },
          {
            "command": "kubectl get secret backup-credentials -n backup-system --no-headers | awk '{print $1}'",
            "expected": "backup-credentials",
            "points": 2,
            "description": "Backup credentials should be configured"
          },
          {
            "command": "kubectl get configmap backup-config -n backup-system --no-headers | awk '{print $1}'",
            "expected": "backup-config",
            "points": 2,
            "description": "Backup configuration should exist"
          }
        ]
      },
      {
        "id": "cka-a-011",
        "title": "Configure Advanced Admission Controllers",
        "description": "Implement and configure multiple admission controllers including ValidatingAdmissionWebhook and MutatingAdmissionWebhook with custom logic.",
        "points": 12,
        "timeLimit": 25,
        "category": "Security",
        "tags": ["admission", "webhooks", "validation"],
        "infrastructure": {
          "namespaces": ["webhook-system"],
          "resources": ["deployments", "services", "validatingadmissionconfigurations", "mutatingadmissionconfigurations"],
          "prerequisites": ["TLS certificates"]
        },
        "solution": {
          "steps": [
            "1. Create TLS certificates for webhook servers",
            "2. Deploy mutating admission webhook",
            "3. Deploy validating admission webhook",
            "4. Configure admission controller settings",
            "5. Create custom validation logic",
            "6. Test webhook functionality",
            "7. Verify mutation and validation behaviors"
          ]
        },
        "validations": [
          {
            "command": "kubectl get validatingadmissionconfiguration custom-validator --no-headers | awk '{print $1}'",
            "expected": "custom-validator",
            "points": 6,
            "description": "Validating admission configuration should exist"
          },
          {
            "command": "kubectl get mutatingadmissionconfiguration custom-mutator --no-headers | awk '{print $1}'",
            "expected": "custom-mutator",
            "points": 6,
            "description": "Mutating admission configuration should exist"
          }
        ]
      },
      {
        "id": "cka-a-012",
        "title": "Implement GitOps Workflow",
        "description": "Set up ArgoCD or Flux for GitOps-based cluster management. Configure automated deployments, rollbacks, and multi-environment promotion.",
        "points": 12,
        "timeLimit": 25,
        "category": "Cluster Management",
        "tags": ["gitops", "argocd", "automation"],
        "infrastructure": {
          "namespaces": ["argocd", "dev", "staging", "production"],
          "resources": ["deployments", "services", "applications"],
          "prerequisites": ["Git repository", "container registry"]
        },
        "solution": {
          "steps": [
            "1. Install ArgoCD in the cluster",
            "2. Configure Git repository connections",
            "3. Set up application manifests in Git",
            "4. Create ArgoCD applications for each environment",
            "5. Configure automated sync policies",
            "6. Set up promotion workflows",
            "7. Test GitOps deployment and rollback"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment argocd-server -n argocd --no-headers | awk '{print $1}'",
            "expected": "argocd-server",
            "points": 4,
            "description": "ArgoCD server should be deployed"
          },
          {
            "command": "kubectl get applications -n argocd --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 4,
            "description": "ArgoCD applications should be configured"
          },
          {
            "command": "kubectl get deployment argocd-application-controller -n argocd --no-headers | awk '{print $1}'",
            "expected": "argocd-application-controller",
            "points": 2,
            "description": "ArgoCD application controller should be running"
          },
          {
            "command": "kubectl get service argocd-server -n argocd --no-headers | awk '{print $1}'",
            "expected": "argocd-server",
            "points": 2,
            "description": "ArgoCD server service should be accessible"
          }
        ]
      },
      {
        "id": "cka-a-013",
        "title": "Configure Advanced Logging",
        "description": "Deploy ELK stack (Elasticsearch, Logstash, Kibana) or similar for centralized logging. Configure log aggregation, parsing, and alerting.",
        "points": 12,
        "timeLimit": 25,
        "category": "Monitoring",
        "tags": ["logging", "elasticsearch", "kibana"],
        "infrastructure": {
          "namespaces": ["logging"],
          "resources": ["deployments", "services", "daemonsets", "configmaps"],
          "prerequisites": ["persistent storage"]
        },
        "solution": {
          "steps": [
            "1. Deploy Elasticsearch cluster",
            "2. Deploy Logstash for log processing",
            "3. Deploy Kibana for visualization",
            "4. Configure Fluentd/Fluent Bit for log collection",
            "5. Set up log parsing and enrichment",
            "6. Create Kibana dashboards",
            "7. Configure log-based alerting"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment elasticsearch -n logging --no-headers | awk '{print $1}'",
            "expected": "elasticsearch",
            "points": 4,
            "description": "Elasticsearch should be deployed"
          },
          {
            "command": "kubectl get deployment kibana -n logging --no-headers | awk '{print $1}'",
            "expected": "kibana",
            "points": 3,
            "description": "Kibana should be deployed"
          },
          {
            "command": "kubectl get daemonset fluentd -n logging --no-headers | awk '{print $1}'",
            "expected": "fluentd",
            "points": 3,
            "description": "Fluentd should be deployed as DaemonSet"
          },
          {
            "command": "kubectl get service elasticsearch -n logging --no-headers | awk '{print $1}'",
            "expected": "elasticsearch",
            "points": 2,
            "description": "Elasticsearch service should be accessible"
          }
        ]
      },
      {
        "id": "cka-a-014",
        "title": "Implement Service Mesh",
        "description": "Deploy Istio service mesh with traffic management, security policies, and observability features. Configure mTLS and traffic routing.",
        "points": 15,
        "timeLimit": 30,
        "category": "Networking",
        "tags": ["servicemesh", "istio", "mtls"],
        "infrastructure": {
          "namespaces": ["istio-system", "app"],
          "resources": ["deployments", "services", "virtualservices", "destinationrules"],
          "prerequisites": ["sufficient cluster resources"]
        },
        "solution": {
          "steps": [
            "1. Install Istio control plane",
            "2. Enable automatic sidecar injection",
            "3. Deploy sample applications",
            "4. Configure traffic management rules",
            "5. Enable mTLS for secure communication",
            "6. Set up observability tools (Jaeger, Kiali)",
            "7. Test traffic routing and security policies"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment istiod -n istio-system --no-headers | awk '{print $1}'",
            "expected": "istiod",
            "points": 5,
            "description": "Istio control plane should be deployed"
          },
          {
            "command": "kubectl get deployment istio-proxy -n istio-system --no-headers | awk '{print $1}' 2>/dev/null || echo 'not-found'",
            "expected": "istio-proxy|not-found",
            "points": 3,
            "description": "Istio components should be running"
          },
          {
            "command": "kubectl get virtualservice --all-namespaces --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 4,
            "description": "VirtualServices should be configured"
          },
          {
            "command": "kubectl get destinationrule --all-namespaces --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "DestinationRules should be configured"
          }
        ]
      },
      {
        "id": "cka-a-015",
        "title": "Configure Disaster Recovery",
        "description": "Implement comprehensive disaster recovery plan including cluster recreation, data restoration, and service continuity across regions.",
        "points": 15,
        "timeLimit": 30,
        "category": "Cluster Maintenance",
        "tags": ["dr", "recovery", "continuity"],
        "infrastructure": {
          "namespaces": ["dr-system"],
          "resources": ["backups", "scripts", "documentation"],
          "prerequisites": ["multi-region setup", "backup systems"]
        },
        "solution": {
          "steps": [
            "1. Document current cluster configuration",
            "2. Create automated backup procedures",
            "3. Set up cross-region replication",
            "4. Create cluster recreation scripts",
            "5. Implement data restoration procedures",
            "6. Set up monitoring for DR readiness",
            "7. Test complete disaster recovery scenario"
          ]
        },
        "validations": [
          {
            "command": "kubectl get configmap dr-procedures -n dr-system --no-headers | awk '{print $1}'",
            "expected": "dr-procedures",
            "points": 5,
            "description": "DR procedures should be documented"
          },
          {
            "command": "kubectl get secret dr-credentials -n dr-system --no-headers | awk '{print $1}'",
            "expected": "dr-credentials",
            "points": 3,
            "description": "DR credentials should be configured"
          },
          {
            "command": "kubectl get cronjob dr-backup -n dr-system --no-headers | awk '{print $1}'",
            "expected": "dr-backup",
            "points": 4,
            "description": "Automated DR backup should be scheduled"
          },
          {
            "command": "kubectl get deployment dr-monitor -n dr-system --no-headers | awk '{print $1}'",
            "expected": "dr-monitor",
            "points": 3,
            "description": "DR monitoring should be active"
          }
        ]
      },
      {
        "id": "cka-a-016",
        "title": "Implement Zero-Downtime Upgrades",
        "description": "Configure and execute rolling upgrades of cluster components with zero downtime. Include upgrade validation and rollback procedures.",
        "points": 12,
        "timeLimit": 25,
        "category": "Cluster Maintenance",
        "tags": ["upgrade", "rolling", "validation"],
        "infrastructure": {
          "namespaces": [],
          "resources": ["nodes", "deployments"],
          "prerequisites": ["multi-node cluster", "load balancer"]
        },
        "solution": {
          "steps": [
            "1. Plan upgrade strategy and validation steps",
            "2. Backup cluster state and etcd",
            "3. Upgrade control plane components sequentially",
            "4. Upgrade worker nodes with rolling strategy",
            "5. Validate cluster functionality at each step",
            "6. Monitor application availability",
            "7. Document upgrade procedures and rollback plan"
          ]
        },
        "validations": [
          {
            "command": "kubectl version --short | grep Server | awk '{print $3}' | grep -c 'v1.28'",
            "expected": "1",
            "points": 6,
            "description": "Cluster should be upgraded to target version"
          },
          {
            "command": "kubectl get nodes --no-headers | awk '{print $2}' | grep -c Ready",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "All nodes should be Ready after upgrade"
          },
          {
            "command": "kubectl get pods --all-namespaces --field-selector=status.phase!=Running --no-headers | wc -l",
            "expected": "0",
            "points": 3,
            "description": "All pods should be running after upgrade"
          }
        ]
      },
      {
        "id": "cka-a-017",
        "title": "Configure Advanced Scheduling",
        "description": "Implement complex scheduling scenarios with pod affinity/anti-affinity, topology spread constraints, and custom schedulers.",
        "points": 10,
        "timeLimit": 20,
        "category": "Scheduling",
        "tags": ["scheduling", "affinity", "topology"],
        "infrastructure": {
          "namespaces": ["scheduling-test"],
          "resources": ["deployments", "pods"],
          "prerequisites": ["multi-zone cluster"]
        },
        "solution": {
          "steps": [
            "1. Label nodes with zones and instance types",
            "2. Create deployments with pod affinity rules",
            "3. Configure anti-affinity for high availability",
            "4. Implement topology spread constraints",
            "5. Test scheduling behavior across zones",
            "6. Verify constraint enforcement",
            "7. Document scheduling policies"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment ha-frontend -n scheduling-test -o jsonpath='{.spec.template.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[0].labelSelector.matchLabels.app}'",
            "expected": "ha-frontend",
            "points": 4,
            "description": "Deployment should have pod anti-affinity configured"
          },
          {
            "command": "kubectl get deployment zone-spread -n scheduling-test -o jsonpath='{.spec.template.spec.topologySpreadConstraints[0].topologyKey}'",
            "expected": "topology.kubernetes.io/zone",
            "points": 3,
            "description": "Deployment should have topology spread constraints"
          },
          {
            "command": "kubectl get pods -n scheduling-test -l app=ha-frontend -o jsonpath='{.items[*].spec.nodeName}' | tr ' ' '\\n' | sort | uniq | wc -l",
            "expected": "^[2-9][0-9]*$",
            "points": 3,
            "description": "HA frontend pods should be spread across multiple nodes"
          }
        ]
      },
      {
        "id": "cka-a-018",
        "title": "Implement Advanced Security Scanning",
        "description": "Deploy security scanning tools for vulnerabilities, compliance, and runtime security. Configure automated scanning and alerting.",
        "points": 12,
        "timeLimit": 25,
        "category": "Security",
        "tags": ["security", "scanning", "compliance"],
        "infrastructure": {
          "namespaces": ["security-system"],
          "resources": ["deployments", "daemonsets", "configmaps"],
          "prerequisites": ["container registry access"]
        },
        "solution": {
          "steps": [
            "1. Deploy vulnerability scanning tools",
            "2. Configure image scanning policies",
            "3. Set up runtime security monitoring",
            "4. Implement compliance scanning",
            "5. Configure automated scanning schedules",
            "6. Set up security alerting",
            "7. Create security dashboards and reports"
          ]
        },
        "validations": [
          {
            "command": "kubectl get deployment security-scanner -n security-system --no-headers | awk '{print $1}'",
            "expected": "security-scanner",
            "points": 4,
            "description": "Security scanner should be deployed"
          },
          {
            "command": "kubectl get daemonset runtime-security -n security-system --no-headers | awk '{print $1}'",
            "expected": "runtime-security",
            "points": 4,
            "description": "Runtime security agent should be deployed"
          },
          {
            "command": "kubectl get configmap security-policies -n security-system --no-headers | awk '{print $1}'",
            "expected": "security-policies",
            "points": 2,
            "description": "Security policies should be configured"
          },
          {
            "command": "kubectl get cronjob compliance-scan -n security-system --no-headers | awk '{print $1}'",
            "expected": "compliance-scan",
            "points": 2,
            "description": "Automated compliance scanning should be scheduled"
          }
        ]
      },
      {
        "id": "cka-a-019",
        "title": "Configure Multi-Tenancy",
        "description": "Implement secure multi-tenant cluster with namespace isolation, resource quotas, network policies, and RBAC for different teams.",
        "points": 12,
        "timeLimit": 25,
        "category": "Security",
        "tags": ["multitenancy", "isolation", "quotas"],
        "infrastructure": {
          "namespaces": ["team-a", "team-b", "team-c", "shared"],
          "resources": ["namespaces", "resourcequotas", "networkpolicies", "roles"],
          "prerequisites": ["multiple teams"]
        },
        "solution": {
          "steps": [
            "1. Create isolated namespaces for each tenant",
            "2. Configure resource quotas per tenant",
            "3. Set up network policies for isolation",
            "4. Create RBAC for team-specific access",
            "5. Configure pod security standards per tenant",
            "6. Set up monitoring and alerting per tenant",
            "7. Test isolation and resource limits"
          ]
        },
        "validations": [
          {
            "command": "kubectl get resourcequota -n team-a --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Team A should have resource quotas"
          },
          {
            "command": "kubectl get networkpolicy -n team-a --no-headers | wc -l",
            "expected": "^[1-9][0-9]*$",
            "points": 3,
            "description": "Team A should have network policies"
          },
          {
            "command": "kubectl get role team-a-admin -n team-a --no-headers | awk '{print $1}'",
            "expected": "team-a-admin",
            "points": 3,
            "description": "Team A admin role should exist"
          },
          {
            "command": "kubectl auth can-i list pods --as=system:serviceaccount:team-a:default -n team-b && echo 'allowed' || echo 'denied'",
            "expected": "denied",
            "points": 3,
            "description": "Team A should not access Team B resources"
          }
        ]
      },
      {
        "id": "cka-a-020",
        "title": "Implement Performance Optimization",
        "description": "Optimize cluster performance through resource tuning, caching, and efficient scheduling. Include performance monitoring and alerting.",
        "points": 12,
        "timeLimit": 25,
        "category": "Performance",
        "tags": ["performance", "optimization", "tuning"],
        "infrastructure": {
          "namespaces": ["performance"],
          "resources": ["deployments", "configmaps", "horizontalpodautoscalers"],
          "prerequisites": ["monitoring system"]
        },
        "solution": {
          "steps": [
            "1. Analyze current cluster performance metrics",
            "2. Optimize node resource allocation",
            "3. Configure CPU and memory limits optimally",
            "4. Implement efficient caching strategies",
            "5. Optimize container images and startup times",
            "6. Configure autoscaling for optimal performance",
            "7. Set up performance monitoring and alerting"
          ]
        },
        "validations": [
          {
            "command": "kubectl get hpa performance-app -n performance --no-headers | awk '{print $1}'",
            "expected": "performance-app",
            "points": 3,
            "description": "Performance-optimized HPA should exist"
          },
          {
            "command": "kubectl get configmap performance-config -n performance --no-headers | awk '{print $1}'",
            "expected": "performance-config",
            "points": 3,
            "description": "Performance configuration should exist"
          },
          {
            "command": "kubectl top nodes | tail -n +2 | awk '{print $3}' | sed 's/%//' | awk '{sum+=$1} END {print (sum/NR < 80) ? \"optimized\" : \"overloaded\"}'",
            "expected": "optimized",
            "points": 3,
            "description": "Cluster CPU utilization should be optimized"
          },
          {
            "command": "kubectl get deployment performance-app -n performance -o jsonpath='{.spec.template.spec.containers[0].resources.limits.cpu}'",
            "expected": "^[0-9]+m$",
            "points": 3,
            "description": "Performance app should have optimized resource limits"
          }
        ]
      }
    ]
  }
}